{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LabanPoseTransformation_Simplified.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skore11/Laban_Pose_Conditional_GAN/blob/main/LabanPoseTransformation_Simplified.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUbEATYXlEHP"
      },
      "source": [
        "!pip install aiohttp nest_asyncio tqdm c3d numpy scipy torch matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf84aVyp8M7K"
      },
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import requests\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "#from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "from zipfile import ZipFile\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
        "#cuda = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vox5OcIbqJtF"
      },
      "source": [
        "# Given a 2D Matrix a, sort and compute the diffs of the rows.\n",
        "# Only keep the row if the diff is greater than the\n",
        "# tolerance\n",
        "# Returns the index array corresponding to the rows to keep\n",
        "def rough_unique(a, tol=1.2):\n",
        "    i = np.argsort(a, axis=0)[:,0]\n",
        "    d = np.append(tol*2, np.mean(abs(np.diff(a[i], axis=0)), axis=1))\n",
        "    return i[(d/a.shape[1])>tol]\n",
        "\n",
        "DATA_ZIP = \"./local_data.zip\" # where the data is downloaded to\n",
        "DATA_DIR = \"./local_data\" # where the data is unzipped to\n",
        "def get_data_zip(\n",
        "        url=\"https://cyprus-data.s3.us-east-2.amazonaws.com/local_data.zip\",\n",
        "        chunk_size=8192,\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Ensure that dataset is downloaded and unzipped\n",
        "    \"\"\"\n",
        "    if not os.path.isfile(DATA_ZIP): # not already downloaded\n",
        "        filesize = int(requests.head(url).headers[\"Content-Length\"])\n",
        "        progress = tqdm(\n",
        "            unit=\"B\",\n",
        "            unit_scale=True,\n",
        "            unit_divisor=1024,\n",
        "            total=filesize,\n",
        "            desc=DATA_ZIP)\n",
        "        r = requests.get(url, stream=True)\n",
        "        with open(DATA_ZIP, 'wb') as fp:\n",
        "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "                wrote = fp.write(chunk)\n",
        "                progress.update(wrote)\n",
        "            progress.close()\n",
        "    if not os.path.isdir(DATA_DIR): # not already unzipped\n",
        "        with ZipFile(DATA_ZIP, 'r') as zp:\n",
        "            files = zp.namelist()\n",
        "            progress = tqdm(\n",
        "                total=len(files),\n",
        "                desc=\"Unzipping {}\".format(DATA_DIR))\n",
        "            for name in files:\n",
        "                zp.extract(name)\n",
        "                progress.update()\n",
        "            progress.close()\n",
        "    return\n",
        "\n",
        "def cyprus_dataset():\n",
        "    \"\"\"\n",
        "    Loads our augmented version of the Cyprus Dataset,\n",
        "    returns two TensorDatasets, one for train and one for test\n",
        "    \"\"\"\n",
        "    get_data_zip() # Ensure data is downloaded\n",
        "    MAX_ZEROS = 3 # max number of 0's a single pose can have\n",
        "    if not os.path.isdir(DATA_DIR):\n",
        "        raise Exception(\"{} does not exist, make sure the\\\n",
        "                        data is downloaded and unzipped\".format(DATA_DIR))\n",
        "\n",
        "    # Get files from dataset\n",
        "    npys = []\n",
        "    for file in os.walk(DATA_DIR):\n",
        "        npys = file[2]\n",
        "\n",
        "    # Load the data\n",
        "    labels_map = {\n",
        "        'afraid':  0,\n",
        "        'bored':   1,\n",
        "        'excited': 2,\n",
        "        'neutral': 3,\n",
        "        'relaxed': 4,\n",
        "    }\n",
        "    poses = np.array([])\n",
        "    labels = np.array([])\n",
        "    for npy in tqdm(npys, desc=\"Loading data\"):\n",
        "        data = np.load(os.path.join(DATA_DIR,npy)).astype('float32')\n",
        "        if data.shape[0] == 0 or data.shape[1] != N_DATA:\n",
        "            continue\n",
        "        # Remove similar poses\n",
        "        uis = rough_unique(data, 2.0)\n",
        "        data = data[uis]\n",
        "\n",
        "        # Augment data with random scale and translations, both\n",
        "        # together and separately\n",
        "        f = 4\n",
        "        rand_scale = lambda: np.random.uniform(low=0.8,high=1.2,size=(data.shape[0],1))\n",
        "        rand_trans = lambda: \\\n",
        "            np.repeat(\n",
        "                np.random.uniform(low=-100.0,high=100.0,size=(data.shape[0],3)),\n",
        "            38, axis=1)\n",
        "        scale = np.vstack([data*rand_scale() for i in range(f)])\n",
        "        #trans = np.vstack([data+rand_trans() for i in range(f)])\n",
        "        #both = np.vstack([(data*rand_scale())+rand_trans() for i in range(f)])\n",
        "        data = np.vstack([data])\n",
        "\n",
        "        # Remove similar poses\n",
        "        #uis = rough_unique(data)\n",
        "        #data = data[uis]\n",
        "\n",
        "        label = labels_map[npy.split(\"_\")[0]]\n",
        "\n",
        "        if poses.any(): # there already is data, add to it\n",
        "            poses = np.vstack([poses,data])\n",
        "            # repeat label for all samples in file\n",
        "            labels = np.concatenate([\n",
        "                labels,\n",
        "                np.repeat(label,data.shape[0])], axis=None)\n",
        "        else: # first record, create arrays\n",
        "            poses = np.array(data)\n",
        "            labels = np.repeat(label,data.shape[0])\n",
        "\n",
        "    # Remove similar poses\n",
        "    uis = rough_unique(poses,2.0)\n",
        "    poses = poses[uis]\n",
        "    labels = labels[uis]\n",
        "\n",
        "    # Shuffle\n",
        "    shuf = np.random.permutation(poses.shape[0])\n",
        "    poses = poses[shuf]\n",
        "    labels = labels[shuf]\n",
        "\n",
        "    cutoff = int(poses.shape[0] * 0.8)\n",
        "    train_poses = torch.tensor(poses[:cutoff].astype('float32'), device=device)\n",
        "    train_labels = torch.tensor(labels[:cutoff].astype('float32'), device=device)\n",
        "    test_poses = torch.tensor(poses[cutoff:].astype('float32'), device=device)\n",
        "    test_labels = torch.tensor(labels[cutoff:].astype('float32'), device=device)\n",
        "    return TensorDataset(train_poses, train_labels), TensorDataset(test_poses, test_labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T-hWy3Gj1c1"
      },
      "source": [
        "N_POINTS = 38*3\n",
        "N_FEATURES = 0\n",
        "N_DATA = N_POINTS+N_FEATURES\n",
        "N_CLASSES = 5\n",
        "N_NOISE = 1024\n",
        "\n",
        "def block(i, o, norm=True):\n",
        "    layers =[nn.Linear(i, o)]\n",
        "    if norm:\n",
        "        layers.append(nn.BatchNorm1d(o, 0.8))\n",
        "    layers.append(nn.PReLU())\n",
        "    return layers\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    Similar to Generator and Discriminator from Generative Tweening\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.label_embedding = nn.Embedding(N_CLASSES, N_CLASSES)\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(N_DATA+N_NOISE+N_CLASSES, 256, norm=False),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            *block(1024, 2048),\n",
        "            nn.Dropout(0.2),\n",
        "            *block(2048, 2048),\n",
        "            nn.Dropout(0.2),\n",
        "            *block(2048, 1024),\n",
        "            *block(1024, 512),\n",
        "            *block(512, 256),\n",
        "            *block(256, N_DATA, norm=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, pose, label, noise):\n",
        "        x = torch.cat((pose, self.label_embedding(label), noise), -1)\n",
        "        y = self.model(x)\n",
        "        return y\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.label_embedding = nn.Embedding(N_CLASSES, N_CLASSES)\n",
        "        self.model = nn.Sequential(\n",
        "            *block(N_DATA+N_CLASSES, 256, norm=False),\n",
        "            *block(256, 512, norm=False),\n",
        "            nn.Dropout(0.5),\n",
        "            *block(512, 512, norm=False),\n",
        "            nn.Dropout(0.5),\n",
        "            *block(512, 512, norm=False),\n",
        "            nn.Dropout(0.5),\n",
        "            *block(512, 1024, norm=False),\n",
        "            nn.Dropout(0.5),\n",
        "            *block(1024, 1024, norm=False),\n",
        "            *block(1024, 1, norm=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, gen_pose, label):\n",
        "        x = torch.cat((gen_pose, self.label_embedding(label)), -1)\n",
        "        y = self.model(x)\n",
        "        return y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V3HMvhUj1c1"
      },
      "source": [
        "# For general setup and training approach:\n",
        "#   https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/cgan/cgan.py\n",
        "\n",
        "# Training inits\n",
        "batch_size = 2**9 if cuda else 2**6\n",
        "epochs = 5000\n",
        "lr = 0.001\n",
        "b1 = 0.5\n",
        "b2 = 0.999\n",
        "\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "adversarial_loss = nn.MSELoss()\n",
        "\n",
        "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
        "\n",
        "# If GPU available, init for cuda\n",
        "generator.to(device)\n",
        "discriminator.to(device)\n",
        "adversarial_loss.to(device)\n",
        "\n",
        "# Create optimizers after moving model to GPU\n",
        "#opt_G = torch.optim.Adam(generator.parameters(), lr=0.0008, betas=(b1, b2))\n",
        "#opt_D = torch.optim.Adam(discriminator.parameters(), lr=0.0016, betas=(b1, b2))\n",
        "opt_G = torch.optim.RMSprop(generator.parameters(), lr=0.0005)\n",
        "opt_D = torch.optim.RMSprop(discriminator.parameters(), lr=0.0010)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26H1ledSj1c1"
      },
      "source": [
        "# Load data, display pie chart to show split\n",
        "train, test = cyprus_dataset()\n",
        "\n",
        "# Count occurences of each label\n",
        "fig, axs = plt.subplots(2, 1)\n",
        "fig.suptitle(\"Training/Test split for {:,} frames\".format(len(train) + len(test)))\n",
        "train_count = {}\n",
        "for _, label in tqdm(train, desc=\"Analyzing training data\"):\n",
        "    train_count[int(label)] = train_count.get(int(label), 0) + 1\n",
        "\n",
        "pie_labels = ['afraid','bored','excited','neutral','relaxed']\n",
        "pcts = [train_count[k] / len(train) for k in sorted(train_count.keys())]\n",
        "axs[0].pie(\n",
        "    pcts,\n",
        "    labels=[pie_labels[k] + \" {:.2f}%\".format(pcts[k]*100) for k in sorted(train_count.keys())],\n",
        "    normalize=True)\n",
        "axs[0].set_title(\"Train Data ({:,} frames)\".format(len(train)))\n",
        "\n",
        "test_count = {}\n",
        "for _, label in tqdm(test, desc=\"Analyzing testing data\"):\n",
        "    test_count[int(label)] = test_count.get(int(label), 0) + 1\n",
        "\n",
        "pcts = [test_count[k] / len(test) for k in sorted(test_count.keys())]\n",
        "axs[1].pie(\n",
        "    pcts,\n",
        "    labels=[pie_labels[k] + \" {:.2f}%\".format(pcts[k]*100) for k in sorted(test_count.keys())],\n",
        "    normalize=True)\n",
        "axs[1].set_title(\"Test Data ({:,} frames)\".format(len(test)))\n",
        "#plt.show()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"./data_pie_chart.png\")\n",
        "plt.close()\n",
        "print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFuHv-5Vj1c1"
      },
      "source": [
        "dl = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "print(\"Training started: {:,} total poses, batch size {:,}\".format(len(train), batch_size))\n",
        "op = tqdm(range(epochs), position=1, desc=\"Epoch\")\n",
        "\n",
        "g_losses = np.array([])\n",
        "d_losses = np.array([])\n",
        "d_real_losses = np.array([])\n",
        "d_fake_losses = np.array([])\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "for epoch in op:\n",
        "    ip = tqdm(\n",
        "        enumerate(dl),\n",
        "        position=2,\n",
        "        total=int(len(train)/batch_size)+1,\n",
        "        desc=\"[G loss: ???] [D loss: ???]\",\n",
        "        leave=False)\n",
        "\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "    for i, (data, labels) in ip:\n",
        "        valid = FloatTensor(np.full((data.shape[0],1), 1.0)) # label smoothing\n",
        "        fake = FloatTensor(np.full((data.shape[0],1), 0.0))\n",
        "\n",
        "        # Train generator\n",
        "        opt_G.zero_grad()\n",
        "        noise = FloatTensor(np.random.normal(0, 1, (data.shape[0], N_NOISE)))\n",
        "        gen_labels = labels[torch.randperm(data.shape[0])].long()\n",
        "        gen_output = generator(data, gen_labels, noise)\n",
        "\n",
        "        # How well are we fooling the discriminator\n",
        "        # (matching the desired label)?\n",
        "        validity = discriminator(gen_output, gen_labels)\n",
        "        g_loss = adversarial_loss(validity, valid)\n",
        "\n",
        "        g_loss.backward()\n",
        "        opt_G.step()\n",
        "\n",
        "        # Train Discriminator\n",
        "        opt_D.zero_grad()\n",
        "\n",
        "        # How well can the discriminator label real poses?\n",
        "        real_validity = discriminator(data[:,:N_POINTS], labels.long())\n",
        "        d_real_loss = adversarial_loss(real_validity, valid)\n",
        "\n",
        "        # Can the discriminator detect the generated poses?\n",
        "        fake_validity = discriminator(gen_output.detach(), gen_labels.long())\n",
        "        d_fake_loss = adversarial_loss(fake_validity, fake)\n",
        "\n",
        "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "        d_loss.backward()\n",
        "        opt_D.step()\n",
        "\n",
        "        # Update progress bar\n",
        "        ip.set_description(\n",
        "            desc=\"[G loss: %f] [D loss: %f]\"\n",
        "                % (g_loss.item(), d_loss.item())\n",
        "        )\n",
        "\n",
        "        # Track metrics\n",
        "        g_losses = np.append(g_losses, g_loss.item())\n",
        "        d_losses = np.append(d_losses, d_loss.item())\n",
        "        d_real_losses = np.append(d_real_losses, d_real_loss.item())\n",
        "        d_fake_losses = np.append(d_fake_losses, d_fake_loss.item())\n",
        "\n",
        "    ip.close()\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        generator.eval()\n",
        "        noise = FloatTensor(np.random.normal(0, 1, (5, N_NOISE)))\n",
        "        gen_input = data[0].repeat(5,1)\n",
        "        gen_output = generator(\n",
        "            gen_input, LongTensor(np.array([0,1,2,3,4])), noise).cpu().detach().numpy()\n",
        "        np.save(\"output_{}.npy\".format(epoch), np.vstack([\n",
        "            data[0].cpu().detach().numpy()[:114],\n",
        "            gen_output]))\n",
        "\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'generator_state_dict': generator.state_dict(),\n",
        "            'discriminator_state_dict': discriminator.state_dict(),\n",
        "            'opt_G_state_dict': opt_G.state_dict(),\n",
        "            'opt_D_state_dict': opt_D.state_dict(),\n",
        "            'g_loss': g_loss,\n",
        "            'd_loss': d_loss,\n",
        "            }, \"checkpoint_{}.pt\".format(epoch))\n",
        "\n",
        "    l = g_losses.shape[0]\n",
        "    o = 500\n",
        "    if epoch != epochs-1:\n",
        "        if (len(train)//batch_size)*epoch > o:\n",
        "            xs = np.arange(l-o,l)\n",
        "            plt.plot(xs, g_losses[-o:], label=\"G Losses\")\n",
        "            plt.plot(xs, d_losses[-o:], label=\"D Losses\")\n",
        "            plt.plot(xs, d_real_losses[-o:], label=\"D Real Losses\")\n",
        "            plt.plot(xs, d_fake_losses[-o:], label=\"D Fake Losses\")\n",
        "            plt.xlabel(\"Training step (Batch)\")\n",
        "            plt.ylabel(\"Loss\")\n",
        "            plt.legend()\n",
        "            #plt.show()\n",
        "            plt.savefig(\"losses_checkpoint.png\")\n",
        "            plt.close()\n",
        "            np.save(\"g_losses_checkpoint.npy\", g_losses)\n",
        "            np.save(\"d_losses_checkpoint.npy\", d_losses)\n",
        "        else:\n",
        "            xs = np.arange(l)\n",
        "            plt.plot(xs, g_losses, label=\"G Losses\")\n",
        "            plt.plot(xs, d_losses, label=\"D Losses\")\n",
        "            plt.plot(xs, d_real_losses, label=\"D Real Losses\")\n",
        "            plt.plot(xs, d_fake_losses, label=\"D Fake Losses\")\n",
        "            plt.xlabel(\"Training step (Batch)\")\n",
        "            plt.ylabel(\"Loss\")\n",
        "            plt.legend()\n",
        "            #plt.show()\n",
        "            plt.savefig(\"losses_checkpoint.png\")\n",
        "            plt.close()\n",
        "            np.save(\"g_losses_checkpoint.npy\", g_losses)\n",
        "            np.save(\"d_losses_checkpoint.npy\", d_losses)\n",
        "\n",
        "torch.save({\n",
        "    'epoch': epoch,\n",
        "    'generator_state_dict': generator.state_dict(),\n",
        "    'discriminator_state_dict': discriminator.state_dict(),\n",
        "    'opt_G_state_dict': opt_G.state_dict(),\n",
        "    'opt_D_state_dict': opt_D.state_dict(),\n",
        "    'g_loss': g_loss,\n",
        "    'd_loss': d_loss,\n",
        "    }, \"final_state.pt\")\n",
        "np.save(\"g_losses_final.npy\", g_losses)\n",
        "np.save(\"d_losses_final.npy\", d_losses)\n",
        "print(\"Saved models!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alf42pjMj1c1"
      },
      "source": [
        "# TODO: display poses\n",
        "# TODO: Play with losses + optimizers\n",
        "# TODO: speed up training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc1K53ABj1c1"
      },
      "source": [
        "xs = np.arange(g_losses.shape[0])\n",
        "plt.plot(xs, g_losses, label=\"G Losses\")\n",
        "plt.plot(xs, d_losses, label=\"D Losses\")\n",
        "plt.plot(xs, d_real_losses, label=\"D Real Losses\")\n",
        "plt.plot(xs, d_fake_losses, label=\"D Fake Losses\")\n",
        "plt.xlabel(\"Training step (Batch)\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "#plt.show()\n",
        "plt.savefig(\"losses_final.png\")\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaVQZhR-j1c1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}